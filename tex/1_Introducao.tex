Codificação e compressão de imagens é um grande desafio no campo de processamento de imagens. Para entender melhor a complexidade e a necessidade de se resolver esse desafio, este capítulo apresenta uma motivação do tema na seção \ref{sec:motivacao}; na seção \ref{sec:hipotese} é apresentada a hipótese; os objetivos gerais e específicos são propostos na seção \ref{sec:objetivos} e  resultados esperados na seção \ref{sec:expectativa}.

\section{Motivação}
\label{sec:motivacao}
A codificação de dados é a transformação feita nos dados para atingir um certo objetivo, como compressão ou criptografia. O principal objetivo dos algoritmos de compressão é a redução do comprimento da mensagem (codificação da fonte), enquanto a criptografia tem como foco transformar os dados para proteger sigilo ou integridade daquilo que el es significam, e/ou acesso a tais dados, durante a sua transmissão através de um canal vulnerável.

Compressão de dados é o processo de codificar uma determinada informação utilizando uma menor representação. Os dois principais benefícios trazidos pela compressão de dados são o aumento significativo na capacidade de armazenamento de um sistema e menor largura de banda necessária para transmití-los.  

De forma sucinta, compressão de dados é a arte ou ciência de representar informação de forma compacta~\cite{sayood2017introduction}. Nós criamos essas representações compactas identificando e usando estruturas que existem nos dados para que seja possível extrair redundância dos dados e descrevê-la em forma de um modelo que será usado como base para a codificação~\cite{sayood2017introduction}. 

O desenvolvimento de algoritmos de compressão de dados podem ser divididos em duas fases~\cite{sayood2017introduction}. A primeira fase é geralmente chamada de modelagem. Nessa fase, tentamos extrair informações sobre qualquer redundância existente nos dados e descrevemos a redundância na forma de um modelo. A segunda fase é chamada de codificação: uma descrição do modelo e uma ``descrição'' de como os dados diferem do modelo são codificados, geralmente usando um alfabeto binário. A diferença entre os dados e o modelo é frequentemente referida como resíduo.

Existem dois tipos de compressão~\cite{sayood2017introduction}: com perdas e sem perdas. A compressão com perdas (\textit{lossy}) potencializa uma melhor taxa de compressão em troca de perda de informação enquanto na compressão sem perdas (\textit{lossless}) não há perda de informação. Esta última é requerida em algumas aplicações, como sinais biomédicos. No contexto de imagens digitais, a compressão sem perdas permite que, após a codificação da imagem, a imagem decodificada seja idêntica à original, enquanto na compressão com perdas a imagem decodificada não é idêntica à original e há perda de qualidade visual.

Durante um processo de compressão devemos balancear dois pontos: a capacidade de compressão (taxa), isto é, o tamanho final em bits da imagem comprimida, e a distorção, que é a diferença entre a imagem original e a imagem reconstruída. Essa otimização é representada pela equação: $$ J = D(B) + \lambda R(B), $$ onde $D(B)$ representa a distorção entre a imagem original e a reconstruída e $R(B)$ a quantidade de bits usada para representar a imagem, $\lambda$ é um parâmetro adimensional e $B$ é o \textit{bitstream} gerado pela imagem.

O objetivo em codificar uma imagem é representá-la com o menor número possível de bits, preservando a qualidade e a inteligibilidade necessárias à sua aplicação de modo a facilitar sua transmissão e armazenamento. Ou seja, busca-se minimizar $J$. São utilizadas medidas de desempenho para a codificação sem perdas e com perdas que diz respeito a taxa de compressão e distorção. Uma das formas de medir distorção comumente utilizada em processamento de imagens é o erro médio quadrático (\acrshort{MSE})\footnote{MSE também é bastante utilizada como função de custo para modelos de regressão}: \equacao{mse}{\frac{1}{n}\sum_{n=1}^{N}{(\mathbf{x}(n) - \hat{\mathbf{x}}(n))}^2, \text{ onde }\, \mathbf{x}\, \text{ representa a imagem original e }\, \hat{\mathbf{x}}\, \text{ a imagem decodificada.}}
    
A princípio, uma maior quantidade de bits implicaria numa distorção $D(B)$ menor, mas resultaria numa taxa $R(B)$ maior. Contudo, esse é na verdade um problema intratável, como mostrado por~\cite{shoham1988efficient}. Para uma dada taxa, existem diversas representações com distorções melhores e piores.

% Falando sobre compressão de imagens e motivação social
O motivo pelo qual precisamos de usar compressão de dados é porque estamos gerando e usando cada vez mais dados digitais. O número de \textit{bytes} necessários para representados dados multimídia pode ser enorme. Por exemplo, para representar digitalmente 1 segundo de vídeo sem compressão usando o formato CCIR 601~\cite{sayood2017introduction}, é necessário mais do que 20 MB para armazenar ou 160 Mb para transmitir~\cite{sayood2017introduction}. Considerando o número de segundos em um filme, é fácil ver porque compressão é necessárias à determinadas aplicações. Para serviços de streaming de mídia como Netflix, não usar compressão não é uma opção.

Algoritmos de compressão de imagens aproveitam da percepção visual e propriedades estatísticas de dados da imagem para fornecer resultados superiores quando comparados com métodos de compressão de dados genéricos, que são usados para outros dados digitais. A tarefa de compressão de imagens foi cuidadosamente examinada durante anos por pesquisadores e times como o \textit{Joint Pictures Experts Group} que desenvolveram os métodos de compressão de imagens JPEG~\cite{jpeg1993} e JPEG2000~\cite{jpeg2000}. Mais recentemente, o algoritmo WebP~\cite{webp} foi proposto para melhorar as taxas de compressão em imagens de alta resolução, que vem sendo cada vez mais utilizadas. O codec (codificador e decodificador) do estado da arte atual é o BPG~\cite{bpg}.

Assim como os outros codecs, o \textit{JPEG} explora as características imperfeitas da nossa percepção. Ele foi o primeiro padrão internacional de compressão para imagens monocromáticas e coloridas. Até hoje é um padrão bastante utilizado e possui métodos para compressão com perdas (baseado em transformada discreta de cosseno) e sem perdas (método preditivo). Para criar um arquivo JPEG, primeiro a imagem é convertida para outro espaço de cor: o \textit{YCbCr}. Esse espaço, que é usado em vários vídeos de alta definição, codifica a cor de uma forma diferente do RGB, apesar de cobrir as mesmas cores. Os componentes Cb e Cr (crominância azul e vermelha, respectivamente) são altamente compressíveis, visto que o sistema visual é capaz de discriminar o brilho de uma imagem com muito mais sensibilidade do que sua informação de cor\footnote{Existem cerca de 120 milhões de bastonetes (células estimuladas pelo brilho) distribuídos sobre a superfície da retina~\cite{olho} contra apenas 6 à 7 milhões de cones (células estimuladas pela cor). Essa diferença no número de bastonetes se dá por motivos evolutivos pois era mais importante identificar possíveis predadores ou presas durante a noite do que identificar cor}. Isto significa que os valores dos componentes de luminância precisam de muito mais fidelidade que os componentes de crominância.

Os algoritmos de compressão existentes atualmente podem estar longe de serem os ideais para os novos formatos de mídia como vídeos em 360 graus ou conteúdos de realidade virtual. Enquanto um desenvolvimento de um novo codec pode levar anos, um \textit{framework} de compressão de imagens mais geral baseado em redes neurais pode ser capaz de se adaptar mais rápido a essas diferentes tarefas e ambientes.

Algoritmos padrão de compressão de imagens tendem a fazer suposições sobre a escala da imagem. Por exemplo, usualmente assume-se que um \textit{patch} (pequeno pedaço retangular da imagem) de uma imagem natural de alta resolução irá conter muita informação redundante. De fato, quanto maior a resolução da imagem, mais provável que a maior parte dos \textit{patches} que a compõem conterão informação de baixa frequência onde não há muita variação nos valores dos pixels. Esse fato é explorado pela maior parte dos codecs de imagens, de modo que eles tendem a ser muito eficientes em comprimir imagens de alta resolução. Entretanto, tais suposições são invalidadas ao criar miniaturas de imagens naturais de alta resolução, visto que um \textit{patch} obtido de uma miniatura pode conter informação de alta frequência que é mais difícil de ser comprimida por estes algoritmos.
Nos últimos anos, redes neurais profundas se tornaram a base dos resultados do estado da arte para categorização de imagens~\cite{simonyan}, detecção de objetos~\cite{girshick2014rich}, reconstrução tridimensional de objetos~\cite{choy20163d}, reconhecimento de faces~\cite{deepface}, reconhecimento de discurso~\cite{graves}, tradução automática~\cite{sequence}, geração de legendas de imagens~\cite{vinyals2015show}, tecnologia de carros autônomos~\cite{huval2015empirical}, entre outros. 

É natural buscar usar essa poderosa classe de métodos para melhorar a tarefa de compressão de imagens, especialmente para imagens que não são cuidadosamente desenvolvidas para codecs otimizados por heurísticas, como miniaturas. Considerando um codificador de imagem como um problema de análise/síntese com uma camada de gargalo no meio, é possível encontrar uma grande área de pesquisa que usa redes neurais para encontrar representações comprimidas. Muito desse trabalho se concentra em uma classe de redes neurais conhecida como \emph{autoencoders}~\cite{autoencoder2011}. Alguns resultados já existentes para compressão com perdas usando autoencoders se mostraram promissores: ~\cite{gregor2016towards, FullResolution2017Toderici, Variable2016Toderici}, e redes neurais já atingiram o estado da arte para compressão sem perdas~\cite{mentzer2019, theis2015generative}. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Hipóteses}
\label{sec:hipotese}
Compressão de imagens usando redes neurais tem sido uma área ativa de pesquisa em tempos recentes com vários desafios a serem enfrentados para que essas técnicas sejam competitivas com os codificadores clássicos. Serão verificadas as seguintes hipóteses:
\begin{itemize}
    \item Se modelos baseados em \textit{autoencoders} convolucionais recorrentes são competitivos com os clássicos codecs \textit{JPEG} e \textit{JPEG2000} para imagens com muito conteúdo de alta frequência;
    \item Se o codificador de entropia utilizado no latente será capaz de comprimir em proporções semelhantes para todos as redes de todos os níveis;
    \item Se ao usar imagens, no treinamento, que codificadores clássicos têm dificuldade para comprimir é benéfico para os resultados dele;
    \item Se há um grande impacto nos resultados ao usar funções de custo variadas;
\end{itemize}

Estas verificações serão dadas usando-se métricas visuais objetivas e as imagens serão trabalhadas a baixas taxas de bits por pixel.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Objetivos}
\label{sec:objetivos}
O objetivo deste trabalho é construir e avaliar o desempenho de um \textit{framework} de compressão de imagens ponta a ponta (modelagem e codificação) usando \textit{autoencoders} empilhados convolucionais recorrentes com o auxílio de um codificador de entropia para comprimir o latente binarizado gerado pelo \textit{encoder}. 

Para isto foram estudadas propostas de compressão de imagens na literatura de modo a obter estatísticas e informações para guiar a escolha e implementação de codificadores baseados em autoencoders que estendam a estrutura básica de um autoencoder, gerando uma representação binária para a imagem ao quantizar a camada de gargalo ou as variáveis latentes correspondentes. Esta representação binária será ainda comprimida usando um codificador de entropia. 

Além disso serão construídas bases de dados próprias, a partir de bases de dados comumente utilizadas para esse problema, para que seja avaliado o desempenho, a diferentes taxas, de \textit{autoencoders} convolucionais e dos codecs \textit{JPEG} e \textit{JPEG2000} nas mais variadas imagens. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Resultados Esperados}
\label{sec:expectativa}
Espera-se obter desempenho superior aos métodos de compressão \textit{JPEG} e \textit{JPEG2000} para imagens com muito conteúdo de alta frequência, visto que estes métodos assumem que baixas frequências são mais visualmente relevantes. Além disso, visto que existem trabalhos semelhantes na literatura acredita-se que será obtido melhor desempenho com o uso de imagens mais difíceis de comprimir para treinamento e com o uso de modelos recorrentes. 

% Compressão em larga escala de imagens é uma aplicação importante, tanto em termos de redução de armazenamento em disco quanto em fazer melhor uso da banda limitada da internet. Várias imagens são atualmente transmitidas ao longo da \textit{web} para prévias de \textit{sites}, galerias de fotos, ferramentas de busca e várias outras aplicações. Uma grande fração do tráfego da internet é dirigido por requisições de dispositivos móveis com telas relativamente pequenas e requerimentos de largura de banda rigorosos. Portanto, aumentar a compressão de imagens além das capacidades dos codificadores existentes é de grande relevância, visto que economizar \textit{bytes} reduz custos, melhoram a experiência dos usuários e possibilitam diversas aplicações.