Codificação e compressão de imagens é um grande desafio no campo de processamento de imagens. Para entender melhor a complexidade e a necessidade de se resolver esse desafio, este capítulo apresenta uma motivação do tema na seção \ref{sec:motivacao}; na seção \ref{sec:hipotese} é apresentada a hipótese; os objetivos gerais e específicos são propostos na seção \ref{sec:objetivos} e  resultados esperados na seção \ref{sec:expectativa}.

\section{Motivação}
\label{sec:motivacao}
A codificação de dados é a transformação feita nos dados para atingir um certo objetivo, como compressão ou criptografia. O principal objetivo dos algoritmos de compressão é a redução do comprimento da mensagem (codificação da fonte), enquanto a criptografia tem como foco transformar os dados para proteger sigilo ou integridade daquilo que el es significam, e/ou acesso a tais dados, durante a sua transmissão através de um canal vulnerável.

Compressão de dados é o processo de codificar uma determinada informação utilizando uma menor representação. Os dois principais benefícios trazidos pela compressão de dados são o aumento significativo na capacidade de armazenamento de um sistema e menor largura de banda necessária para transmití-los.  

De forma sucinta, compressão de dados é arte ou ciência de representar informação de forma compacta~\cite{sayood2017introduction}. Nós criamos essas representações compactas identificando e usando estruturas que existem nos dados para que seja possível extrair redundância dos dados e descrevê-la em forma de um modelo que será usado como base para a codificação~\cite{sayood2017introduction}. 

O desenvolvimento de algoritmos de compressão de dados podem ser divididos em duas fases~\cite{sayood2017introduction}. A primeira fase é geralmente chamada de modelagem. Nesta fase, tentamos extrair informações sobre qualquer redundância existente nos dados e descrevemos a redundância na forma de um modelo. A segunda fase é chamada de codificação. Uma descrição do modelo e uma ``descrição'' de como os dados diferem do modelo são codificados, geralmente usando um alfabeto binário. A diferença entre os dados e o modelo é frequentemente referida como resíduo.

Existem dois tipos de compressão: com perdas e sem perdas. A compressão com perdas (\textit{lossy}) potencializa uma melhor taxa de compressão em troca de perda de informação enquanto na compressão sem perdas (\textit{lossless}) não há perda de informação. Esta última é requerida em algumas aplicações, como sinais biomédicos. No contexto de imagens digitais, a compressão sem perdas permite que, após a codificação da imagem, a imagem decodificada seja idêntica à original, enquanto na compressão com perdas a imagem decodificada não é idêntica à original e há perda de qualidade visual.

% Falando sobre compressão de imagens e motivação social
O motivo pelo qual precisamos de usar compressão de dados é porque estamos gerando e usando cada vez mais dados digitais. O número de \textit{bytes} necessários para representados dados multimídia pode ser enorme. Por exemplo, para representar digitalmente 1 segundo de vídeo sem compressão usando o formato CCIR 601~\cite{sayood2017introduction}, é necessário mais do que 20 \textit{megabytes} para armazenar ou 160 megabits para transmitir~\cite{sayood2017introduction}. Considerando o número de segundos em um filme, é fácil ver porque compressão é necessárias à determinadas aplicações. Para serviços de streaming de mídia como Netflix, não usar compressão não é uma opção.

Os recursos necessários para armazenar e transmitir imagens são imensos, o que torna a sua compressão necessária. O objetivo em codificar uma imagem é representá-la com o menor número possível de bits, preservando a qualidade e a inteligibilidade necessárias à sua aplicação de modo a facilitar sua transmissão e armazenamento. São utilizadas medidas de desempenho para a codificação sem perdas e com perdas que diz respeito a taxa de compressão e distorção. Uma das formas de medir distorção comumente utilizada em processamento de imagens é o \acrshort{MSE}\footnote{MSE também é bastante utilizada como função de loss para modelos de aprendizado profundo baseados em redes neurais}: \equacao{mse}{\frac{1}{n}\sum_{n=1}^{N}{(x(n) - \hat{x}(n))}^2, \text{ onde }\, x\, \text{ representa a imagem original e }\, \hat{x}\, \text{ a imagem decodificada}}

Algoritmos de compressão de imagens aproveitam da percepção visual e propriedades estatísticas de dados da imagem para fornecer resultados superiores quando comparados com métodos de compressão de dados genéricos, que são usados para outros dados digitais. A tarefa de compressão de imagens foi cuidadosamente examinada durante anos por pesquisadores e times como o \textit{Joint Pictures Experts Group} que desenvolveram os métodos de compressão de imagens JPEG~\cite{jpeg1993} e JPEG2000~\cite{jpeg2000}. Mais recentemente, o algoritmo WebP~\cite{webp} foi proposto para melhorar as taxas de compressão em imagens de alta resolução, que vem sendo cada vez mais utilizadas. O codec (codificador e decodificador) do estado da arte atual é o BPG~\cite{bpg}.

Assim como os outros codecs, o \textit{JPEG} explora as características imperfeitas da nossa percepção. Ele foi o primeiro padrão internacional de compressão para imagens monocromáticas e coloridas. Até hoje é um padrão bastante utilizado e possui métodos para compressão com perdas (método baseado em transformada discreta de cosseno) e sem perdas (método preditivo). Para criar um arquivo JPEG, primeiro a imagem é convertida para outro espaço de cor: o \textit{YCbCr}. Este espaço, que é usado em vários vídeos de alta definição, codifica a cor de uma forma diferente do RGB, apesar de cobrir as mesmas cores. Os componentes Cb e Cr (crominância azul e vermelha, respectivamente) são altamente compressíveis, enquanto o componente de luminância indica quão brilhante o pixel é. 

Na superfície da retina existem dois tipos de células que contêm pigmentos: os cones e os bastonetes. Os bastonetes existem em maior quantidade na periferia da retina e são estimulados com luz de baixa intensidade. Eles servem para dar um quadro geral do campo de visão e não estão envolvidos com a visão colorida (em baixos níveis de luz, nós praticamente não vemos cor, pois a iluminação é muito baixa para estimular os cones da nossa retina). Os cones, por sua vez, são muito sensíveis às cores e ocorrem principalmente na região central da retina. Seu estímulo depende de altas intensidades luminosas. É nessa região que a imagem é formada com maior nitidez, pois são estimulados pela luz mais intensa. Os cones são especializados na acuidade da visão diurna e em reconhecer a cor. O cérebro interpreta os sinais recebidos por esses cones, o que permite processar a diferenciação das cores. O olho humano é capaz de discriminar o brilho de uma imagem muito mais que sua informação de cor, visto que existem cerca de 120 milhões de bastonetes distribuídos sobre a superfície da retina contra apenas 6 à 7 milhões de cones\footnote{Esta diferença no número de bastonetes se dá por motivos evolutivos pois era mais importante identificar possíveis predadores ou presas durante a noite do que identificar cor}. Isto significa que os valores dos componentes de luminância precisam de muito mais fidelidade que os componentes de crominância (o mesmo vale para o componente de cor verde no espaço RGB, por exemplo).

Os algoritmos de compressão existentes atualmente podem estar longe de serem os ideais para os novos formatos de mídia como vídeos em 360 graus ou conteúdos de realidade virtual. Enquanto um desenvolvimento de um novo codec pode levar anos, um \textit{framework} de compressão de imagens mais geral baseado em redes neurais pode ser capaz de se adaptar mais rápido à estas diferentes tarefas e ambientes.

Algoritmos padrão de compressão de imagens tendem a fazer suposições sobre a escala da imagem. Por exemplo, usualmente assume-se que um \textit{patch} (pedaço retangular da imagem) de uma imagem natural de alta resolução irá conter muita informação redundante. De fato, quanto maior a resolução da imagem, mais provável que a maior parte dos \textit{patches} que a compõem conterão informação de baixa frequência onde não há muita variação nos valores dos pixels. Esse fato é explorado pela maior parte dos codecs de imagens, de modo que eles tendem a ser muito eficientes em comprimir imagens de alta resolução. Entretanto, tais suposições são invalidadas ao criar miniaturas de imagens naturais de alta resolução, visto que um \textit{patch} obtido de uma miniatura pode conter informação de alta frequência que é mais difícil de ser comprimida por estes algoritmos.

Nos últimos anos, redes neurais profundas se tornaram a base dos resultados do estado da arte para reconhecimento de imagens~\cite{simonyan}, detecção de objetos~\cite{girshick2014rich}, reconstrução tridimensional de objetos~\cite{choy20163d}, reconhecimento de faces~\cite{deepface}, reconhecimento de discurso~\cite{graves}, \textit{machine translation}~\cite{sequence}, geração de legendas de imagens~\cite{vinyals2015show}, tecnologia de carros autônomos~\cite{huval2015empirical}, entre outros. 

É natural buscar usar essa poderosa classe de métodos para melhorar a tarefa de compressão de imagens, especialmente para imagens que não são cuidadosamente desenvolvidas para codecs otimizados por heurísticas, como miniaturas. Considerando um codificador de imagem como um problema de análise/síntese com uma camada de gargalo no meio, é possível encontrar uma grande área de pesquisa que usa redes neurais para encontrar representações comprimidas. Muito desse trabalho se concentra em uma classe de redes neurais conhecida como \emph{autoencoders}~\cite{autoencoder2011}. Alguns resultados já existentes para compressão com perdas usando autoencoders se mostraram promissores: ~\cite{gregor2016towards, FullResolution2017Toderici, Variable2016Toderici}, e redes neurais já atingiram o estado da arte para compressão sem perdas~\cite{mentzer2019, theis2015generative}. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Hipótese}
\label{sec:hipotese}
Compressão de imagens usando redes neurais tem sido uma área ativa de pesquisa em tempos recentes com vários desafios a serem enfrentados para que essas técnicas sejam competitivas com os codificadores clássicos. Serão verificadas as seguintes hipóteses:
\begin{itemize}
    \item Deseja-se verificar se modelos baseados em \textit{autoencoders} convolucionais são competitivos com os clássicos codecs \textit{JPEG} e \textit{JPEG2000} para imagens com alto conteúdo de alta frequência;
    \item Deseja-se verificar se usar imagens que codificadores clássicos têm dificuldade para comprimir para treinamento do modelo é benéfico para os resultados dele;
    \item Deseja-se verificar se o modelo recursivo é melhor que o modelo não recursivo para a tarefa de compressão de imagens;
    \item Deseja-se verificar se os resultados dos \textit{autoencoders} serão melhores ao trabalhar com um diferente domínio para os dados de entrada, um que beneficie o processo de aprendizagem via atualização de gradientes. Ou seja, se é benéfico para o desempenho do modelo aplicar uma transformação nos dados de entrada que evite problemas como gradientes muito pequenos.
\end{itemize}

Estas verificações serão dadas usando-se métricas visuais objetivas e as imagens serão trabalhadas à baixas taxas de bits por pixel.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Objetivos}
\label{sec:objetivos}
Deseja-se avaliar o desempenho de um \textit{autoencoder} convolucional recorrente e não recorrente na tarefa de compressão de imagens à baixas taxas de bits por pixel.

Para isto serão estudadas propostas de compressão de imagens na literatura, de modo a obter estatísticas e informações para guiar a escolha e implementação de codificadores baseados em autoencoders que estendam a estrutura básica de um autoencoder, gerando uma representação binária para a imagem ao quantizar a camada de gargalo ou as variáveis latentes correspondentes. Serão construídas bases de dados próprias, a partir de bases de dados comumente utilizadas para esse problema, para que seja avaliado o desempenho, à diferentes taxas, de \textit{autoencoders} convolucionais e dos codecs \textit{JPEG} e \textit{JPEG2000} nas mais variadas imagens.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Resultados Esperados}
\label{sec:expectativa}
Acredita-se que será obtido desempenho superior aos métodos de compressão \textit{JPEG} e \textit{JPEG2000} para imagens com alto conteúdo de alta frequência, visto que estes métodos assumem que baixas frequências são mais visualmente e perceptualmente relevantes. Além disso, visto que existem trabalhos semelhantes na literatura acredita-se que será obtido melhor desempenho com o uso de imagens mais difíceis de comprimir para treinamento e com o uso de modelos recorrentes. 

% Compressão em larga escala de imagens é uma aplicação importante, tanto em termos de redução de armazenamento em disco quanto em fazer melhor uso da banda limitada da internet. Várias imagens são atualmente transmitidas ao longo da \textit{web} para prévias de \textit{sites}, galerias de fotos, ferramentas de busca e várias outras aplicações. Uma grande fração do tráfego da internet é dirigido por requisições de dispositivos móveis com telas relativamente pequenas e requerimentos de largura de banda rigorosos. Portanto, aumentar a compressão de imagens além das capacidades dos codificadores existentes é de grande relevância, visto que economizar \textit{bytes} reduz custos, melhoram a experiência dos usuários e possibilitam diversas aplicações.