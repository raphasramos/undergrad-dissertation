\newcommand{\texCommand}[1]{\texttt{\textbackslash{#1}}}%

\newcommand{\exemplo}[1]{%
\vspace{\baselineskip}%
\noindent\fbox{\begin{minipage}{\textwidth}#1\end{minipage}}%
\\\vspace{\baselineskip}}%

\newcommand{\exemploVerbatim}[1]{%
\vspace{\baselineskip}%
\noindent\fbox{\begin{minipage}{\textwidth}%
#1\end{minipage}}%
\\\vspace{\baselineskip}}%

Este capítulo descreve conceitos fundamentais de codecs clássicos e trabalhos similares utilizadas como base para este trabalho.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Métodos clássicos de compressão}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Arquivos comprimidos pelo clássico codec JPEG normalmente são descritos no formato \acrshort{JFIF}, que é uma limitação,(um subconjunto) do padrão JPEG completo. 
O método de compressão JPEG descrito na~\refFig{jpeg}, assim como os outros métodos, exploram a 
imperfeição do sistema visual humano. Os 5 passos usados para codificação no padrão JFIF são descritos nas subseções seguintes.
\figura[htb]{jpeg}{Diagram do método de compressão JPEG. Fonte:~\cite{jpeg1993}}{jpeg}{width=\textwidth}
\subsection{Conversão do espaço de cor}
Para o padrão \acrshort{JFIF}\footnote{O método completo JPEG permite que qualquer espaço de cor (organização específica de cores que permite representações de cores) seja usado, mas a maior parte das pessoas seguiram com o espaço de cor definido no padrão \acrshort{JFIF} por praticidade.}, primeiro, é feita a conversão do espaço RGB da imagem de entrada para o espaço de cor \textit{YCbCr}. Os valores dos pixels estarão no intervalo de 0 à 255 (mesmo do RGB).
Uma vez que o espaço de cor é transformado para \textit{YCbCr}, é necessário decidir qual será o fator usado para reduzir a quantidade de pixels nos componentes de crominância, visto que o olho humano é muito mais sensível ao brilho do que a cor. Normalmente é usado um fator de 2 nas duas direções, o que dá 4 vezes menos cor de modo que para cada 4 pixels Y da imagem só terá 1 pixel Cb e 1 Cr no \textit{bitstream} correspondente da imagem codificada. Assim, quando o \textit{decodificador} do \textit{codec} lê o \textit{bitstream} que representa a imagem comprimida, os pixels restantes serão inferidos e não necessariamentes serão iguais aos originais. Esse fator é determinado pelo argumento \textit{quality} passado como parâmetro para codificação da imagem (com \textit{quality} máxima, não haverá redução e a imagem possuirá a mesma resolução de cor).

Nota-se que na~\refFig{image_comparison} praticamente não há diferença visual, mesmo a codificação da imagem da direita (\textit{bitstream}) possuindo 64 vezes menos pixels de crominância (Cb e Cr) do que a imagem da esquerda. Entretanto, olhando a~\refFig{zoom_image} é possível notar certa discrepância nas bordas da arara vermelha.
\figura[!htb]{image_comparison}{Imagem original sem compressão retirada de~\cite{kodak} e imagem (direita), gerada a partir da imagem original, com dimensionalidade nos canais de crominância reduzida por um fator de 8 nas duas direções. Pode-se jogar informação da imagem original fora ao codificar e então usar a imagem da direita para armazenamento ou transmissão, que terá novos valores em seus canais de cores (aumenta-se a dimensionalidade apenas quando for necessário exibí-la). Fonte:~\cite{kodak}}{image_comparison}{width=\textwidth}
\figura[!htb]{zoom_image}{Versão com zoom das imagens mostradas na~\refFig{image_comparison}. A região mostrada é exatamente a mesma (espacialmente) para as duas imagens, com a mesma quantidade de pixels. Imagem original sem compressão (esquerda) e imagem (direita) cuja codificação (\textit{bitstream}) possui 64 vezes menos pixels de crominância (Cb e Cr) menos cor da imagem original}{zoom_image}{width=\textwidth}

\subsection{Aplicação da transformada direta de cossenos}
Nesse passo é feita a divisão da imagem em blocos com 8 pixels de largura e 8 de altura que serão convertidos em uma nova matriz com o auxílio de uma transformada discreta de cossenos \acrshort{DCT}~\cite{ahmed1974discrete}. Essa transformação, que é similar a transformada de \textit{Fourier}, analisa as frequências dos valores originais dos pixels da imagem ao longo de cada linha e coluna usando um conjunto de ondas cossenos oscilando em diferentes frequências e amplitudes. Cada um dos blocos serão transformados separadamente e podem ser exatamente replicados por ondas de cossenos 8 por 8, onde varia-se frequências e amplitudes de cada uma delas. 

A representação de um sinal em \acrshort{DCT} tende a ter maior parte da sua energia (definimos energia de um sinal como sendo: $\sum_{k = - \infty}^{\infty}{|x_k|}^2$) concentrada em um número menor de coeficientes quando comparado com outras transformações como a transformação discreta de \textit{Fourier} (\acrshort{DFT}), o que é mais desejável para algoritmos de compressão. Além disso, a \acrshort{DFT} tem coeficientes complexos, o que dobra a quantidade de bits necessários para a representação do sinal. 

Basicamente, na transformada discreta de cossenos são calculados os coeficientes das ondas de cossenos. Os coeficientes podem ser considerados como a quantidade relativa das frequências espaciais 2D contidas no sinal de entrada. O coeficiente com frequência zero nas duas dimensões é chamado de coeficiente corrente direto (\acrshort{DC}) e os 63 restantes são chamados de coeficientes correntes alternados (\textit{AC}). O decodificador reverte este passo usando a função inversa da DCT (\acrshort{IDCT}) que pega os 64 coeficientes \textit{Forward DCT} (\acrshort{FDCT}) do codificador já quantizados e reconstrói o sinal da imagem de 64 pontos. Se a \acrshort{FDCT} e a \acrshort{IDCT} pudessem ser computadas com acurácia perfeita e se os coeficientes da \acrshort{DCT} não fossem quantizados no codificador, o sinal original de 64 pontos poderia ser exatamente recuperado.

A~\refFig{DCT} mostra as 64 funções de cosseno que podem ser combinadas para formar qualquer imagem 8 por 8. Nota-se que a partir do bloco superior esquerdo, as frequências das ondas de cosseno crescem tanto na direção horizontal quanto na vertical. Além disso, o bloco inferior direito constituído de um padrão de xadrez, é o que possui maior frequência. Para criar qualquer imagem 8 por 8, basta combinar todos esses blocos ao mesmo tempo. Cada um será ponderado baseado em um número denonimado coeficiente que representará a contribuição de cada um desses blocos individuais para o todo. Assim, se a contribuição de um bloco for zero não haverá nenhuma parte desta função de cossenos na imagem 8 por 8 buscada. 

\figura[!htb]{DCT-8x8}{64 (8 por 8) ondas base de cossenos com frequências variadas. Fonte:~\cite{fig:dct}}{DCT}{width=\textwidth}

Mudanças de altas frequências podem ser minimizadas ou zeradas, visto que nós não percebemos suas mudanças na imagem tão bem quanto componentes de baixas frequência. Ou seja, blocos de imagens cujos valores de pixels mudam de intensidade muito rápido (normalmente perto das bordas da imagem) podem ser borrados sem perda significativas de qualidade visual, o que economiza uma quantidade enorme de espaço. Por isso, os coeficientes das ondas de cossenos de alta frequência não contribuem muito para a imagem final. Entretanto, isso não é verdade para textos, o que faz com que o JPEG não seja uma boa escolha quando o objetivo é comprimir imagens de texto, conforme mostra a~\refFig{text_jpeg}.
\figura[!htb]{text_jpeg}{Comparação imagem de texto com e sem compressão. Fonte:~\cite{fig:text_jpeg}}{text_jpeg}{width=\textwidth}
\subsection{Quantização}
O propósito da quantização é alcançar mais compressão ao representar os coeficientes \acrshort{DCT} com a menor precisão possível para alcançar a qualidade da imagem especificada. Isso é feito descartando informação que não é visualmente significante.

Quantização é definida dividindo cada coeficiente pelo passo do quantizador especificado na tabela de quantização, seguido por um arredondamento para o inteiro mais próximo. Na dequantização é feito o processo inverso multiplicando pelo passo do quantizador, com o auxílio da mesma tabela usada para quantização. Quantização é o passo em que há a maior perda de informação na imagem. É dada pela seguinte equação:
\equacao{quant}{\round{F^Q(u,v) = \dfrac{F(u,v)}{Q(u,v)}}}
onde $F^Q(u,v)$ será o novo valor do coeficiente, $F(u,v)$  é o valor atual e $Q(u,v)$ é o passo de quantização que controla a qualidade da imagem definido para a aplicação (altas frequências são removidas usando um valor maior para $Q(u,v)$. Cada um dos 64 coeficientes \acrshort{DCT} são uniformemente quantizados em conjunto com uma tabela de quantização de 64 elementos, que é definida pelo nível de qualidade escolhido para a aplicação.

Após a quantização, os coeficientes \acrshort{DC} são tratados separadamente devido à alta correlação destes coeficientes em blocos 8 por 8 adjacentes da imagem, considerando que eles geralmente possuem maior valor e muito impacto na imagem. Assim, eles são codificados como a diferença do coeficiente DC do bloco anterior na ordem de codificação. Por fim, todos os coeficientes quantizados são ordenados em uma sequência zig-zag conforme mostra a~\refFig{zigzag} com o objetivo de facilitar a codificação (que será usada no próximo passo) ao ordenar os coeficientes de frequência similares próximos uns dos outros.
\figura[!htb]{zig-zag}{Sequência zig-zague usada para melhorar codificação. Fonte:~\cite{jpeg1993}}{zigzag}{width=0.7\textwidth}

Vale ressaltar que existe apenas um controle de qualidade pelas matrizes de quantização e não uma otimização da taxa-distorção $J$.
\subsection{Codificação}
O passo final do codificador é a codificação de entropia responsável por gerar o \textit{bitstream} comprimido que representará a imagem. Este passo permite compressão sem perdas adicional ao codificar os coeficientes \acrshort{DCT} quantizados de forma mais compacta baseando-se em suas características estatísticas. O \textit{JPEG} propõe o uso de dois métodos de codificação de entropia: Codificador de Huffman~\cite{huffman1952method} e Codificador Aritmético~\cite{pennebaker1988arithmetic}. 

No final, para cada bloco 8 por 8 da imagem original, teremos três matrizes 8 por 8 quantizadas, onde as matrizes correspondentes aos canais \textit{Cb} e \textit{Cr} serão as mais comprimidas.

Terminada a codificação, o papel do decodificador será de reverter os passos do codificador para reconstruir a imagem, conforme mostra a~\refFig{jpeg}: primeiro, a matriz quantizada será obtida decodificando os blocos comprimidos. Depois, é possível obter a matriz \acrshort{DCT} multiplicando a matriz quantizada pela matriz de quantização utilizada pelo codificador. Posteriormente, essa matriz é transformada usando a \acrshort{IDCT} que resultará na matriz no espaço de cor \textit{YCbCr}.

O \textit{JPEG2000} tem um funcionamento geral similar ao JPEG, entretanto é utilizado uma transformada discreta \textit{wavelet} no lugar da \acrshort{DCT}. Esta trasformada é aplicada na imagem inteira, o que elimina o efeito de \textit{blocking} causado pelo \textit{JPEG} mas causa o efeito de \textit{ring}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Redes Neurais}
Algumas tarefas em inteligência artificial podem ser resolvidas desenvolvendo características a serem extraídas dos dados. Entretanto, para muitas tarefas é difícil saber quais \textit{features} (informações incluída nas representações dos dados) devem ser extraídas. Uma solução para esse problema é descobrir não apenas a função que mapeará a entrada para a saída mas também a própria representação. Essa abordagem é conhecida como \textit{representation learning} (aprendizado de representações)~\cite{deeplearning}.

\textit{Deep learning} (aprendizado profundo) resolve o problema de \textit{representation learning} introduzindo representações que são expressadas em termos de outras representações mais simples~\cite{deeplearning}. Por exemplo, usando um modelo de \textit{deep learning} é possível representar o conceito de uma imagem de um carro combinando conceitos mais simples, como bordas e contornos.

Redes neurais são algoritmos comumente usados em \textit{deep learning} capazes de fazer predições aprendendo uma função que relaciona as características dos dados a respostas observadas/desejadas. Elas aprendem essa função ao tentar minimizar a função de custo/erro (\textit{loss}) definida para a aplicação. Redes neurais são consideradas aproximadores universais de funções, o que significa que elas podem computar e são capazes de aproximar qualquer função (não só lineares)~\cite{dlbook}. Para isso,é necessário que elas sejam profundas o suficiente e possuam funções de ativações (funções não-lineares), visto que a saída de uma rede sem funções de ativação seria apenas uma função linear (polinômio de grau um) que não é capaz de representar algumas funções como a função \acrshort{XOR}~[\refFig{xor}]~\cite{deeplearning}. 

As ativações permitem que o modelo aprenda funções mais complexas, ao introduzir transformações não-lineares nas saídas das camadas. A \textit{Rectified Linear Unit} (\acrshort{ReLU}), definida como $f(x) = \text{max}(0, x)$, é uma das funções de ativações não-lineares mais comuns e recomendadas pois ela é quase linear, o que faz com que o modelo seja facilmente otimizável com métodos comumente usados como descida de gradiente~\cite{rumelhart1988learning}. Métodos de descida de gradiente são métodos que atualizam os pesos com base no gradiente, de modo que a função de erro será minimizada dando passos proporcional ao negativo do gradiente em direção ao ponto mínimo. 

\figura[!htb]{xor}{Um modelo linear aplicado diretamente à entrada original não pode implementar a função \acrshort{XOR}, visto que o espaço da função \acrshort{XOR} não é linearmente separável. Para isso é necessário transformar o espaço original usando uma função de ativação. Fonte:~\cite{deeplearning}}{xor}{width=\textwidth}

\subsection{Taxa de aprendizagem (\textit{learning rate})}
A \textit{learning rate} ($l$) é um hiperparâmetro que controla o quanto nós ajustamos os parâmetros aprendíveis da nossa rede com respeito ao gradiente $g$. Quanto menor o seu valor, menor será a influência do gradiente obtido em direção à menor função de custo. A \textit{learning rate} é um dos hiperparâmetros que devem ser escolhidos com cuidado,pois ela pode ter uma grande influência na convergência do seu modelo, visto que os novos pesos $n$ serão calculados através da seguinte equação: \equacao{peso}{n = o - lg}, onde $o$ representa os pesos antigos.

O gráfico~\refFig{lr} mostra os diferentes cenários de \textit{learning rate} e como ela afeta o treinamento.
\figura[!htb]{lr}{Efeitos de várias taxas de aprendizagem no treinamento. Fonte:~\cite{fig:lr}}{lr}{width=0.6\textwidth}
Leslie N. Smith argumenta em~\cite{smith2017cyclical} que é benéfico para a aprendizagem variar a \textit{learning rate} de forma cíclica durante o treinamento para evitar cair em mínimos locais não ótimos (pontos de sela). Também é mostrado que é possível atingir resultados iguais ou superiores com menos iterações de treinamento usando este método quando comparado com uma rede que foi treinada com \textit{learning rate} fixa ou usando outro método padrão de variação (este fênomeno ficou conhecido como ``superconvergência'').

\textit{Smith} propõe ciclos para variar a \textit{learning rate}. Um ciclo é definido como o número de iterações necessárias para \textit{learning rate} ir do valor mínimo até o máximo definido no ciclo e voltar ao mínimo. Dadas as constantes $baselr,\, maxlr,\, step \text{ e } \gamma$ que representam, respectivamente, \textit{learning rate} inicial, \textit{learning rate} máxima, número de iterações correspondente a metade de um ciclo e constante responsável por diminuir limite superior do ciclo; e as variáveis $itr$ que representa a iteração atual no treinamento e $cycle = \left\lfloor1 + \dfrac{itr}{2 \cdot step}\right\rfloor$ o ciclo atual, a \textit{learning rate} $lr$ para uma $itr$ qualquer na política \textit{exp\_range}~[\refFig{exp}], é calculada pela seguinte equação:
\equacao{lr_eq}{lr = baselr + (maxlr-baselr)\text{max}\max(0,\, 1-|itr/step - 2cycle + 1|) \gamma^{itr}.}
\figura[!htb]{exp}{Política \textit{exp\_range} de \textit{learning rate} cíclica. Fonte:~\cite{smith2017cyclical}}{exp}{width=\textwidth}

Pode-se usar um conjunto de validação para salvar e usar os pesos do modelo que obter a melhor métrica no conjunto de validação, pois o modelo que obtiver a menor função de erro no treinamento não necessariamente será aquele que obterá a melhor métrica no conjunto de teste usado para avaliação do modelo. Para que este objetivo seja atingido, normalmente são usados otimizadores. O método clássico de descida do gradiente estocástico (\acrshort{SGD}) consiste basicamente em usar amostras aleatórias (\textit{mini-batch} para aproximar o verdadeiro gradiente que levará à minimização da função de custo/erro escolhida. O \acrshort{SGD} mantém uma única \textit{learning rate} (não muda durante o treino) para todas as atualizações de peso. O \textit{Adam}~\cite{kingma2014adam} é um otimizador que adapta a \textit{learning rate}baseando-se na média do primeiro momento e do segundo momento dos gradientes e na média móvel exponencial do gradiente e da raiz quadrada dele. Os parâmetros utilizados neste otimizador controlam as taxas de decaimento destas médias móveis. Ele é um método mais caro, portanto não há necessidade de usar políticas de \textit{learning rate} adaptativas com ele.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Redes Neurais Convolucionais}
Em \textit{deep learning} (aprendizado profundo), uma rede neural convolucional (\acrshort{CNN})~\cite{lecun2010convolutional} é uma classe de redes neurais profundas que usa convoluções para detectarem características e padrões presentes nas imagens. As primeiras camadas detectam características que podem ser reconhecidas e interpretadas de maneira relativamente fácil. Camadas posteriores detectam características mais abstratas e usualmente presentes em muitas das características detectadas por camadas anteriores. A arquitetura de uma \acrshort{CNN} é análoga ao padrão de conectividade dos neurônios no cérebro humano e foi inspirada pela organização do córtex visual. Neurônios individuais respondem a estímulos apenas em uma região restrita do campo visual que é conhecida como campo receptivo. Uma coleção de sobreposição desses campos cobre toda a área visual~\cite{livrodl}.

% \figura[!htb]{convnet}{Exemplo mostrando a saída de cada camada de uma rede neural convolucional. Fonte:~\cite{fig:conv}}{convnet}{width=\textwidth}

Uma \acrshort{CNN} é capaz de capturar dependências espaciais e temporais na imagem de forma bem-sucedida através da aplicação de filtros relevantes e dispensa a necessidade de engenharia de características. A arquitetura realiza um melhor ajuste ao conjunto de imagens devido a redução do número de parâmetros envolvidos e a reusabilidade dos pesos. Em outras palavras, a rede pode ser treinada para entender melhor a complexidade da imagem e suas características relevantes. Essa extração de características é feita por meio da aplicação de filtros\footnote{Filtro se refere a aceitar ou rejeitar certos componentes de frequência no domínio da frequência~\cite{}} no domínio do espaço denominados convoluções. 
% Para uma máscara (filtro) de tamanho $m \times n$, $m = 2a + 1$ e $n = 2b + 1$, onde $a$ e $b$ são inteiros positivos. Para qualquer ponto $(x,\, y)$ na imagem $f$, a resposta do filtro é a soma dos produtos dos coeficientes do filtro e dos pixels da imagem englobados pelo filtro~(\refEq{conv:eq1}).
% \equacao{conv:eq1}{g(x,\, y) \,=\, \sum_{s\, =\, -a}^{a}\, \sum_{t\, =\, -b}^{b}{w(s,t)f(x + s, y + t)}}
% Conforme mostra a~\refFig{conv}, $x$ e $y$ variam de modo que cada pixel em $w$ visite todos os pixels em $f$.
% \figura[!htb]{conv}{Ilustração da operação de filtragem no domínio do espaço (convolução). $w$ é o kernel do filtro e $f$ é a área da imagem coberta pelo filtro. Fonte:~\cite{fig:zaghetto}}{conv}{width=\textwidth}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Autoencoders}
\label{ae_coding}
Um Autoencoder (\acrshort{AE}) é uma técnica de aprendizado não-supervisionado na qual pode-se utilizar redes neurais para a tarefa de aprendizado de representações. Um \textit{autoencoder} é formado por duas redes conectadas: um \textbf{encoder} e um \textbf{decoder}.
\begin{itemize}
\item O \textbf{encoder} tem como função converter a informação da entrada em uma representação menor e mais densa chamada de espaço latente. Pode ser representado como uma função de $\mathbf{x}$, $f(\mathbf{x}) = h$, onde $\mathbf{x}$ é a imagem original.
\item O \textbf{decoder}, por sua vez, tenta reconstruir a informação original, passando do espaço latente criado pelo encoder para o espaço original da informação. Pode ser representado como uma função de $h$, $g(h) = \hat{x}$
\end{itemize} 
Uma rede do tipo \acrshort{AE} pode ser descrita como $g(f(\mathbf{x})) = \hat{\mathbf{x}}$. Normalmente, o objetivo é apenas diminuir a diferença entre $\mathbf{x}$ e $\hat{\mathbf{x}}$ (nesse caso, a função de custo a ser minimizada normalmente é a $MSE(\mathbf{x}, \hat{\mathbf{x}})$~[\refEq{mse}]). A camada entre o \textit{encoder} e o \textit{decoder}, onde os dados de entrada são representados em uma dimensionalidade menor~[\refFig{autoencoder}] e que força o \textit{encoder} a comprimir informação da representação original gerando o espaço latente, é denominada \textit{bottleneck} (camada de gargalo). Considera-se que esta camada faz parte do \textit{encoder}. 
\figura[!htb]{autoencoder}{Ilustração de um autoencoder}{autoencoder}{width=\textwidth}
% O problema fundamental com autoencoders como modelo gerativo é que o espaço latente para qual as entradas são convertidas pode não ser contínuo ou permitem fácil interpolação~[\refFig{latent_space}].
% \figura[!htb]{latent_space}{Exemplo de espaço latente de um dataset de dígitos de numeros. Nota-se que, devido à descontinuidades, otimizar um AE apenas pelo erro de reconstrução não é bom quando é necessário fazer mais do que apenas replicar a mesma imagem, como gerar novas imagens ou alterações delas. Fonte:~\cite{fig:latent_space}}{latent_space}{width=\textwidth}

Para o problema de compressão de imagens, normalmente usa-se um binarizador na camada de gargalo com o objetivo de binarizar o latente gerado pelo \textit{encoder}. Assim, o \textit{encoder} é forçado a comprimir informação e o \textit{decoder} a diminuir a distorção usando menos informação. O binarizador transforma os valores em ponto flutuante (representação limitada dos números reais no computador) em inteiros que serão binarizados.
Como a entropia é dada pela fórmula $H = -\sum_{i=1}^N P(i)log_2P(i)$, onde $N$ é o comprimento da sequência gerada por um alfabeto e $P(i)$ é a probabilidade que a sequência $i$ ocorra, o binarizador é necessário pois os códigos práticos precisam ter entropia finita ($P(X=X_i)$ para um variável aleatória $X$ contínua é 0, o que faz com que o logaritmo seja ``infinito negativo''). Logo valores contínuos precisam ser quantizados para um conjunto finito de valores discretos, o que introduz erro. Com a binarização também é possível reduzir o espaço consumido pela imagem codificada, visto que números em pontos flutuante com precisão simples ocupam 32 bits o que levaria a uma alta taxa de bits por pixel. A avaliação dos modelos usados para este tipo de problema é dada considerando não só a taxa, mas também métricas visuais que calculam o nível de distorção da imagem reconstruída. Existem três tipos de métricas visuais comumente utilizadas:
\begin{enumerate}
    \item \textit{\acrshort{PSNR}} (Peak Signal-to-Noise Ratio) definida por \equacao{psnr}{20 \cdot \left(\dfrac{MAX_{I}}{\sqrt{MSE}}\right),} onde $MAX$ indica o maior valor possível para o pixel em uma imagem. Quando estes são representados em bits, usa-se $MAX_I = 2^B - 1$;
    \item \textit{\acrshort{SSIM}}~\cite{wang2004image}. Seja $\mathbf{x} = \{x_i|i = 1,2,\dots,N\}$ e $\mathbf{y} = \{y_i|i = 1,2,\dots,N\}$ dois sinais discretos não negativos e $\mu_x, \sigma_x^2$ e $\sigma_{xy}$ serem a média de $\mathbf{x}$, a variância de $\mathbf{x}$ e a covariância de $\mathbf{x}$ e $\mathbf{y}$, respectivamente. A \acrshort{SSIM} é dada por:
    \equacao{SSIM}{SSIM(x, y) = \dfrac{(2\mu_x\mu_y + C_1)(2\sigma_{xy} + C_2)}{(\mu_x^2 + \mu_y^2 + C_1)(\sigma_x^2 + \sigma_y^2 + C_2)},} onde $C_1 = {(K_1L)}^2$, $C_2 = {(K_2L)}^2$ e $C_3 = C_2/2$. $L$ é o intervalo dinâmico dos valores dos pixels ($L = 255$ para 8 bits por pixel) e $K_1, K_2$ são constantes.
    %$$l(x,y) = \dfrac{2\mu_x\mu_y + C_1}{\mu_x^2 + \mu_y^2 + C_1}$$;
    %$$c(x, y) = \dfrac{2\mu_x\mu_y + C_2}{\mu_x^2 + \mu_y^2 + C_2}$$;
    %$$s(x, y) = \dfrac{\mu_{xy} + C_3}{\mu_x\mu_y + C_3}$$;
    \item \textit{\acrshort{MS-SSIM}}~\cite{wang2003multiscale} é uma forma avançada da \acrshort{SSIM}. Ela é conduzida sobre múltiplas escalas através de um processo de múltiplos estágios de processamento em imagens com dimensionalidade reduzida.
    % \item  
    % \equacao{MSSSIM}{MSSSIM(x,y) = {[l_M(x,y)]}^{\alpha M} \cdot \prod_{j=1}^{M}{[c_j(x,y)}^{\beta_j}{[s_j(x,y)]}^{\gamma_j},} onde os expoentes são usados para ajustar a importância relativa de cada componente e $l$, $c$ e $s$ são componentes de luminância, contraste e estrutura, respectivamente.
\end{enumerate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Compressão de Imagens com Taxa Variável usando Redes Neurais Recorrentes}
\label{variablerate}
Nesse trabalho~\cite{Variable2016Toderici} é feita uma abordagem de ponta a ponta (modelagem e codificação) para compressão de imagens. Esta abordagem é baseada em várias redes neurais empilhadas, especificamente, uma pilha de \textit{autoencoders}, o que possibilita a transmissão de informação incremental. Cada autoencoder comprime sua entrada e tenta reconstruí-la. O erro residual é propagado para o próximo \textit{autoencoder} que, novamente, tenta reconstruir seu resíduo de entrada. Considerando \textit{E} como o \textit{encoder}, \textit{D} como o decoder, \textit{B} como a função de binarização, e \textit{x} como a entrada, um \textit{autoencoder} pode ser representado como \equacao{autoenc}{\hat{x} = D(B(E(x))).}
Esaa equação pode ser usada para compor uma pilha de \textit{autoencoders} residuais pelo seguinte conjunto de equações:
\equacao{res}{b_t = B(E_t(r_{t-1})), \, \hat{\mathbf{x}}_t = D_t(b_t) + \gamma\hat{\mathbf{x}}_{t-1},\\r_t = x - \hat{\mathbf{x}}_t, \, r_0 = x, \, \hat{\mathbf{x}}_0 = 0}
onde $t \in \{1,\cdots,n\}$, para um modelo com $n$ níveis. O par $(E_t, D_t)$ é o \textit{autoencoder} para o t-ésimo nível com $r_{t-1}$ como entrada. A entrada inicial é a imagem original. O binarizador $B$, nessa formulação, é o mesmo para todos os níveis de iteração. Usa-se $\gamma = 1$ para a reconstrução aditiva, de modo que a reconstrução final para um modelo com $t$ níveis será igual a $\hat{\mathbf{x}}_t$, que será a soma das saídas de todos os níveis. Para a reconstrução ``one-shot'' usada em modelos com memória, usa-se $\gamma = 0$.

O trabalho~\cite{Variable2016Toderici} propõe uma função de binarização com quantização em dois passos. O primeiro passo é a geração de valores no intervalo $[-1, 1]$. A segunda parte envolve converter esses valores para o conjunto $\{-1, 1\}$. Para esse propósito, uma camada completamente convolucional com ativações de tangente hiperbólica é usada para produzir as saídas no intervalo desejado. Em seguida, uma abordagem estocástica é aplicada. Esses passos podem ser representados pelo seguinte conjunto de equações: \equacao{bin}{b(x) = x + \epsilon \,\in \{-1, 1\},} \equacao{noise}{\epsilon \sim \begin{cases} 
    1 - x\, \text{ com probabilidade } \frac{1+x}{2},\\
    -1 - x\, \text{ com probabilidade } \frac{1-x}{2},\\
    \end{cases}} onde $\epsilon$, corresponde ao ruído de quantização.
    
O encoder pode ser sumarizado pela seguinte equação: \equacao{encoder}{B(x) = b(tanh(Wx + b)).}$W$ e $b$ são os pesos e bias padrões da rede. A quantização é realizada somente na \textit{forward pass}, visto que substituir o arredondamento por uma aproximação completamente pode levar o \textit{decoder} a aprender a inverter essa aproximação, removendo a informação da \textit{bottleneck} (camada de gargalo) que força a rede a comprimir informação. Para a \textit{backward pass} da \textit{back-propagation}, é usada a derivada da esperança de modo que os gradientes serão passados por $b$ sem mudanças pois a esperança de $b(x)$ é igual a $x, \, \forall x \in [-1, 1]$. 

Para ter uma representação fixa para uma entrada, uma vez que a rede esteja treinada $b$ é substituída por \equacao{binf}{b^{inf}(x) = \begin{cases}
-1,\, \text{ se } x < 0,\\
1,\, \text{ caso contrário. }\\
\end{cases}}
\paragraph{Redes Neurais não-recorrentes\\}

Toderici et al.~\cite{Variable2016Toderici} primeiramente propôs um esquema de compressão com esta estratégia usando diferentes arquiteturas de redes neurais. Esse esquema é baseado em um framework de codificação aditivo que restringe o número de bits de codificação. A~\refFig{toderici_1} mostra um exemplo deste empilhamento de \textit{autoencoders} completamente convolucionais. Para essa arquitetura, cada rede é composta de um \textit{encoder}, que produz uma representação latente de 8 bits por pixel e um \textit{decoder} que tenta reconstruir a entrada a partir do latente. Cada camada possui 512 \textit{kernels} com tangente hiperbólica como função de ativação. Os resíduos são as entradas para as próximas redes. Considerando 16 níveis de resíduos, um total de 128 bits é usado, o que dá uma taxa de bits por pixel de $\dfrac{2 \cdot 2 \cdot 32}{32 \cdot 32} = 0.125$ para representar a imagem de entrada de tamanho 32x32. 

\figura[!htb]{toderici_1}{Um \textit{autoencoder} residual \textit{fully-connected}. Esta figura mostra uma arquitetura com dois níveis (dois \textit{autoencoders} empilhados). O primeiro nível codificada a imagem original. O resíduo da reconstrução é passado para o segundo nível. Cada nível produz um latente de 4 bits por pixel, que é representado nos retângulos. Os retângulos marcados com 512 são camadas \textit{fully-connected} com 512 unidades e não-linearidades de tangente hiperbólica. Fonte: ~\cite{Variable2016Toderici}}{toderici_1}{width=\textwidth}

Os autores propõe uma versão iterativa dessa rede usando convoluções em vez de camadas \textit{fully-connected}. A~\refFig{toderici_2} ilustra essa arquitetura. Nesse caso, são utilizados 2 bits por pixel na \textit{bottleneck} binarizada.

\figura[!htb]{toderici_2}{O autoencoder residual convolucional. Os retângulos afiados representam as camadas convolucionais, enquanto os arredondados representam as camadas das convoluções transpostas. A dimensão do kernel é apresentado na primeira linha, enquanto a quantidade de canais de saída e o tamanho do \textit{stride} são apresentados nas linhas seguintes. A loss é aplicada nos resíduos. Fonte: ~\cite{Variable2016Toderici}}{toderici_2}{width=\textwidth}

O conjunto de dados é composto de 216 milhões de images aleatórias coletadas da internet. Então, 90\% destas imagens são usadas para treinamento, enquanto 10\% são usadas para teste (não usa-se conjunto de validação devido ao tamanho enorme de imagens usadas para treinamento). Esse conjunto de dados simula o cenário de compressão de miniaturas de imagens.

O treinamento foi realizado com o otimizador Adam usando uma função de custo normalizada e variando com as seguintes \textit{learning rates} 0.1, 0.3, 0.5, 0.8 e 1. O número de níveis varia de 8 à 16. A \acrshort{SSIM} é usada para avaliar o desempenho em tempo de teste. 

\paragraph{Redes Neurais recorrentes\\}
Além das operações usadas pelas redes não recorrentes, a abordagem recorrente usa camadas recorrentes. Uma rede neural recorrente (\acrshort{RNN}) é um tipo de rede neural com memória para salvar informação sobre entradas passadas. Para isto, estas unidades de memória possuem conexões para elas mesmas. Esta informação temporal muda o comportamento da camada para a entrada atual~\cite{Survey_CSVT2019}.

Para manter informação temporal, muitas implementações das camadas foram propostas. Uma das primeiras abordagens efetivas se espelham na \textit{Long Short-Term Memory (\acrshort{LSTM})}.

A camada \acrshort{LSTM} possui a seguinte formulação, onde $\mathbf{x_t}, \mathbf{c_t} \textrm{ e } \mathbf{h_t}$ denotam a \textit{input}, \textit{cell} e \textit{hidden states} na iteração $t$~\cite{FullResolution2017Toderici}:

\equacao{graves}{
	\begin{aligned}
	{[f, i, o, j]}^{\top} & = [\sigma, \sigma, \sigma, \tanh]^{\top}((Wx_t + Uh_{t-1}) + b) \\
	c_t & = f \odot c_{t-1} + i \odot j \\
	h_t & = o \odot \tanh(c_t),
	\end{aligned}}

\noindent onde $\odot$ é a multiplicação elemento a elemento, $b$ é o bias, $\sigma$ é a função sigmoide, $i$ é a \textit{input gate}, $o$ é a \textit{output gate}, $f$ é a \textit{forget gate} e $j$ é a \textit{input modification gate}. A saída da camada é $h_t$. $W$ e $U$ são transformações lineares convolucionais.

Considerando as camadas de memória descritas por essas equações, a abordagem recursiva pode ser sumarizada usando a seguinte formulação~\cite{FullResolution2017Toderici}:
\equacao{recursive_ae}{
	\begin{aligned}
		b_t = B(E_t(r_{t-1})), \;\hat{x}_t = D_t(b_t) \\
		r_t = x - \hat{x}_t, \;r_0 = x, \;\hat{x}_0 = 0,
	\end{aligned}}

\noindent onde $D_t$ e $E_t$ são o \textit{encoder} e o \textit{decoder} com os estados na iteração $t$, e $b_t$ é a representação binária progressiva. $\hat{\mathbf{x}}_t$ é reconstrução ``one-shot'' e $r_t$ é o resíduo entre $\mathbf{x}$ e a construção $\hat{\mathbf{x}}_t$. Neste caso, $B$ irá produzir uma representação binarizada de tamanho fixo.

No trabalho posterior, Toderici et al.~\cite{FullResolution2017Toderici} mostrou em seus resultados que o efeito de usar um conjunto de treinamento de alta entropia é benéfico para a rede, dada a importância de treinar modelos de \textit{machine learning} com exemplos difíceis. Em processamento de imagens entropia diz respeito à quantidade de informação na imagem (medida da quantidade de informação gerada pela fonte, do ponto de vista de processamento de sinais). Alta entropia pode ser visto como maior variância nos valores dos pixels. No seu trabalho, ele definiu o conjunto de alta entropia como sendo o conjunto formado por imagens que o \acrshort{PNG} tem mais dificuldade de comprimir, ou seja, os arquivos no formato \acrshort{PNG} com o maior número de bytes. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
