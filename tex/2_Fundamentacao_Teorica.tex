\newcommand{\texCommand}[1]{\texttt{\textbackslash{#1}}}%

\newcommand{\exemplo}[1]{%
\vspace{\baselineskip}%
\noindent\fbox{\begin{minipage}{\textwidth}#1\end{minipage}}%
\\\vspace{\baselineskip}}%

\newcommand{\exemploVerbatim}[1]{%
\vspace{\baselineskip}%
\noindent\fbox{\begin{minipage}{\textwidth}%
#1\end{minipage}}%
\\\vspace{\baselineskip}}%

Este capítulo descreve trabalhos similares e conceitos fundamentais de codecs clássicos e arquiteturas baseadas em redes neurais utilizadas neste trabalho.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{JPEG}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\figura[htb]{encoder_jpeg}{Codificador JPEG. Fonte:~\cite{jpeg1993}}{encoder_jpeg}{width=\textwidth}
\figura[htb]{decoder_jpeg}{Decodificador JPEG. Fonte:~\cite{jpeg2000}}{decoder_jpeg}{width=\textwidth}
Arquivos JPEG normalmente são descritos no formato \acrshort{JFIF}~\cite{hamilton2004jpeg}, que é uma limitação do padrão JPEG completo. O método de compressão JPEG descrito na~\refFig{encoder_jpeg}, assim como os outros métodos, exploram a imperfeição do sistema visual humano. Os 5 passos usados para codificação no padrão JFIF são descritos nas subseções seguintes.
\subsection{Conversão do espaço de cor}
Para o padrão \acrshort{JFIF}\footnote{JPEG permite você usar qualquer espaço de cor que queira, mas a maior parte das pessoas seguiram com o padrão \acrshort{JFIF} por praticidade}, primeiro, é feita a conversão do espaço RGB da imagem de entrada para o espaço de cor \textit{YCbCr}. Os valores dos pixels estarão no intervalo de 0 à 255 (mesmo do RGB).
Uma vez que o espaço de cor é transformado para \textit{YCbCr}, é necessário decidir qual será o fator usado para reduzir a quantidade de pixels nos componentes de crominância, visto que o olho humano é muito mais sensível ao brilho do que a cor. Normalmente é usado um fator de 2 nas duas direções, o que dá 4 vezes menos cor (para cada 4 pixels Y só terá 1 pixel Cb e 1 Cr). Esse fator é determinado pelo argumento \textit{quality} passado como parâmetro para codificação da imagem (com \textit{quality} máxima, não haverá redução e a imagem possuirá a mesma resolução de cor).

Nota-se que na~\refFig{image_comparison} praticamente não há diferença visual olhando a um nível normal de zoom, mesmo a codificação da imagem da direita possuindo 64 vezes menos cor do que a imagem da esquerda. Entretanto, olhando a~\refFig{zoom_image} é possível notar certa discrepância nas bordas da arara vermelha.
\figura[!htb]{image_comparison}{Imagem original sem compressão retirada de~\cite{kodak} e imagem (direita), gerada a partir da imagem original, com dimensionalidade nos canais de crominância reduzida por um fator de 8 nas duas direções. Pode-se jogar informação da imagem original fora e então usar a imagem da direita para armazenamento ou transmissão, que terá novos valores em seus canais de cores (aumenta-se a dimensionalidade apenas quando for necessário exibí-la). Fonte:~\cite{kodak}}{image_comparison}{width=\textwidth}
\figura[!htb]{zoom_image}{Versão com zoom das imagens mostradas na~\refFig{image_comparison}. A região mostrada é exatamente a mesma (espacialmente) para as duas imagens, com a mesma quantidade de pixels. Imagem original sem compressão (esquerda) e imagem com 64 vezes menos cor (direita)}{zoom_image}{width=\textwidth}

\subsection{Aplicação da transformada direta de cossenos}
Aqui é feita a divisão da imagem em blocos com 8 pixels de largura e 8 de altura que serão convertidos em uma nova matriz com o auxílio de uma transformada discreta de cossenos \acrshort{DCT}~\cite{ahmed1974discrete} (\textit{JPEG} sempre usa \emph{DCT-II}). Essa transformação, que é similar a transformada de \textit{Fourier}, analisa as frequências dos valores originais dos pixels da imagem ao longo de cada linha e coluna usando um conjunto de ondas cossenos oscilando em diferentes frequências e amplitudes e tentamos representar a imagem usando essas ondas. Cada um dos blocos serão codificados separadamente com sua própria transformada discreta de cossenos e podem ser exatamente replicados por ondas de cossenos 8 por 8, onde varia-se frequências e amplitudes de cada uma delas. 

A~\refFig{DCT} mostra as 64 funções de cosseno que podem ser combinadas para formar qualquer imagem 8 por 8. Nota-se que a partir do bloco superior esquerdo, as frequências das ondas de cosseno crescem tanto na direção horizontal quanto na vertical. Além disso, o bloco inferior direito constituído de um padrão de xadrez, é o que possui maior frequência. Para criar qualquer imagem 8 por 8, basta combinar todos esses blocos ao mesmo tempo. Cada um será ponderado baseado em um número denonimado coeficiente que representará a contribuição de cada um desses blocos individuais para o todo. Assim, se a contribuição de um bloco for zero não haverá nenhuma parte desta função de cossenos na imagem 8 por 8 buscada. Basicamente, na transformada discreta de cossenos é calculado os coeficientes das ondas de cossenos. Os coeficientes podem ser considerados como a quantidade relativa das frequências espaciais 2D contidas no sinal de entrada. O coeficiente com frequência zero nas duas dimensões é chamado de coeficiente corrente direto (\acrshort{DC}) e os 63 restantes são chamados de coeficientes correntes alternados (\textit{AC}). O decodificador reverte este passo usando a função inversa da DCT (\acrshort{IDCT}) que pega os 64 coeficientes \textit{Forward DCT} (\acrshort{FDCT}) do codificador já quantizados e reconstrói o sinal da imagem de 64 pontos somando os sinais base. Se a \acrshort{FDCT} e a \acrshort{IDCT} pudessem ser computadas com acurácia perfeita e se os coeficientes da \acrshort{DCT} não fossem quantizados no codificador, o sinal original de 64 pontos poderia ser exatamente recuperado.

\figura[!htb]{DCT-8x8}{64 (8 por 8) ondas base de cossenos com frequências variadas. Fonte:~\cite{fig:dct}}{DCT}{width=\textwidth}

Mudanças de altas frequências podem ser minimizadas ou zeradas, visto que nós não percebemos suas mudanças na imagem tão bem quanto componentes de baixas frequência. Ou seja, blocos de imagens cujos valores de pixels mudam de intensidade muito rápido (normalmente pertos das bordas da imagem) podem ser borrados sem perda significativas de qualidade visual, o que economiza uma quantidade enorme de espaço. Por isso, os coeficientes das ondas de cossenos de alta frequência não contribuem muito para a imagem final. Entretanto, isso não é verdade para textos, o que faz com que o JPEG não seja uma boa escolha quando o objetivo é comprimir imagens de texto, conforme mostra a~\refFig{text_jpeg}.
\figura[!htb]{text_jpeg}{Comparação imagem de texto com e sem compressão. Fonte:~\cite{fig:text_jpeg}}{text_jpeg}{width=\textwidth}
\subsection{Quantização}
Na quantização é onde temos a maior perda de informação na imagem. Aqui, as altas frequências são removidas usando quantização. Cada um dos 64 coeficientes \acrshort{DCT} são uniformemente quantizados em conjunto com uma tabela de quantização de 64 elementos, que é definida pelo nível de qualidade escolhido para a aplicação. O propósito da quantização é alcançar mais compressão ao representar os coeficientes \acrshort{DCT} com a menor precisão possível para alcançar a qualidade da imagem especificada. Isso é feito descartando informação que não é visualmente significante.

Quantização é definida dividindo cada coeficiente pelo passo do quantizador especificado na tabela de quantização, seguido por um arredondamento para o inteiro mais próximo. Na dequantização é feito o processo inverso com o auxílio da mesma tabela usada para quantização, que significa multiplicar pelo passo do quantizador.

Após a quantização, os coeficientes \acrshort{DC} são tratados separadamente devido à alta correlação destes coeficientes em blocos 8 por 8 adjacentes da imagem, considerando que eles geralmente possuem maior valor e muito impacto na imagem. Assim, eles são codificados como a diferença do coeficiente DC do bloco anterior na ordem de codificação. Por fim, todos os coeficientes quantizados são ordenados em uma sequência zig-zag conforme mostra a~\refFig{zigzag} com o objetivo de facilitar a codificação que será usada no próximo passo, ao ordenar os coeficientes de baixa frequência antes daqueles com alta frequência.
\figura[!htb]{zig-zag}{Sequência zig-zague usada para melhorar codificação. Fonte:~\cite{jpeg1993}}{zigzag}{width=0.7\textwidth}
\subsection{Codificação}
O passo final do codificador é a codificação de entropia. Este passo permite compressão sem perdas adicional ao codificar os coeficientes \acrshort{DCT} quantizados de forma mais compacta baseando-se em suas características estatísticas. O \textit{JPEG} propõe o uso de dois métodos de codificação de entropia: Codificador de Huffman~\cite{huffman1952method} e Codificador Aritmético~\cite{pennebaker1988arithmetic}. No final, para cada bloco 8 por 8 da imagem original, teremos três matrizes 8 por 8 quantizadas, onde as matrizes correspondentes aos canais \textit{Cb} e \textit{Cr} serão as mais comprimidas.

Terminada a codificação, o papel do decodificador será de reverter os passos do codificador para reconstruir a imagem, conforme mostra a~\refFig{decoder_jpeg}: primeiro, a matriz quantizada será obtida decodificando os blocos comprimidos. Depois, é possível obter a matriz \acrshort{DCT} multiplicando a matriz quantizada pela matriz de quantização utilizada pelo codificador. Depois, essa matriz é transformada usando a \acrshort{IDCT} que resultará na matriz no espaço de cor \textit{YCbCr}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Redes Neurais}
Um algoritmo simples de aprendizado de máquina (\textit{machine learning}) chamado regressão logística pode determinar quando recomendar cesariana para uma paciente~\cite{mor1990ranking}. O desempenho desses algoritmos dependem muito da representação dos dados fornecidos. Cada pedaço de informação incluída na representação dos dados é conhecida como \textit{feature} (característica)~\cite{deeplearning}. Algumas tarefas em inteligência artificial podem ser resolvidas desenvolvendo características a serem extraídas dos dados. Entretanto, para muitas tarefas é difícil saber quais \textit{features} devem ser extraídas. Uma solução para esse problema é descobrir não apenas a função que mapeará a entrada para a saída mas também a própria representação. Essa abordagem é conhecida como \textit{representation learning} (aprendizado de representações)~\cite{deeplearning}.

\textit{Deep learning} (aprendizado profundo) resolve o problema de \textit{representation learning} introduzindo representações que são expressadas em termos de outras representações mais simples~\cite{deeplearning}. Por exemplo, usando um modelo de \textit{deep learning} é possível representar o conceito de uma imagem de um carro combinando conceitos mais simples, como bordas e contornos.

Redes neurais são algoritmos de \textit{deep learning} capazes de fazer predições aprendendo uma função que relaciona as características dos dados à respostas observadas/desejadas. Redes neurais são consideradas aproximadores universais de funções, o que significa que elas podem computar e são capazes de aproximar qualquer função (não só lineares)~\cite{dlbook}. Para isso, é necessário que elas sejam profundas o suficiente e possuam funções de ativações (funções não-lineares), visto que a saída de uma rede sem funções de ativação seria apenas uma função linear (polinômio de grau um) que não é capaz de representar algumas funções como a função \acrshort{XOR}~[\refFig{xor}]. As ativações permitem que o modelo aprenda funções mais complexas, ao introduzir transformações não-lineares nas saídas das camadas. A \textit{Rectified Linear Unit} (\acrshort{ReLU}), definida como $f(x) = max(0, x)$, é uma das funções de ativações não-lineares mais comuns e recomendadas pois ela é quase linear, o que faz com que o modelo seja fácil de otimizar com métodos comumente usados como descida de gradiente~\cite{rumelhart1988learning} (método que atualiza os pesos com base no gradiente, de modo que a função de erro será minimizada dando passos proporcional ao negativo do gradiente em direção ao ponto mínimo) e preserva várias propriedades existentes em modelos lineares que permitem que eles generalizem bem~\cite{deeplearning}.
\figura[!htb]{xor}{Um modelo linear aplicado diretamente à entrada original não pode implementar a função \acrshort{XOR}. Para isso é necessário transformar o espaço original usando uma função de ativação. Fonte:~\cite{deeplearning}}{xor}{width=\textwidth}

\subsection{Taxa de aprendizagem (\textit{learning rate})}
A \textit{learning rate} é um hiperparâmetro que controla o quanto nós ajustamos os pesos da nossa rede com respeito ao gradiente. Quanto menor o valor, menor o passo dado ao longo do declive. A \textit{learning rate} é um dos hiperparâmetros que devem ser escolhidos com cuidado, pois ela pode ter uma grande influência na convergência do seu modelo. O gráfico~\refFig{lr} mostra os diferentes cenários de \textit{learning rate} e como ela afeta o treinamento.
\figura[!htb]{lr}{Efeitos de várias taxas de aprendizagem no treinamento. Fonte:~\cite{fig:lr})}{lr}{width=0.6\textwidth}
Leslie N. Smith argumenta em~\cite{smith2017cyclical} que é benéfico para a aprendizagem variar a \textit{learning rate} de forma cíclica durante o treinamento para evitar cair em mínimos locais não ótimos (pontos de sela). Também é mostrado que é possível atingir resultados iguais ou superiores com menos iterações de treinamento usando este método quando comparado com uma rede que foi treinada com \textit{learning rate} fixa ou usando outro método padrão de variação (este fênomeno ficou conhecido como ``superconvergência'').

\textit{Leslie} propõe ciclos para variar a \textit{learning rate}. Um ciclo é definido como o número de iterações necessárias para \textit{learning rate} ir do valor mínimo até o máximo definido no ciclo e voltar ao mínimo. Dadas as constantes $baselr,\, maxlr,\, step \text{ e } \gamma$ que representam, respectivamente, \textit{learning rate} inicial, \textit{learning rate} máxima, número de iterações correspondente à metade de um ciclo e constante responsável por diminuir limite superior do ciclo; e as variáveis $itr$ que representa a iteração atual no treinamento e $cycle = \lfloor 1 + \dfrac{itr}{2 \cdot step} \rfloor$ o ciclo atual, a \textit{learning rate} $lr$ para uma $itr$ qualquer na política \textit{exp\_range}~[\refFig{exp}], é calculada pela seguinte equação:
\equacao{lr}{lr = baselr + (maxlr-baselr) \cdot max(0, 1-|itr/step - 2\cdot cycle + 1|) \cdot \gamma^{itr}.}
\figura[!htb]{exp}{Política \textit{exp\_range} de \textit{learning rate} cíclica. Fonte:~\cite{smith2017cyclical}}{exp}{width=\textwidth}

O objetivo do treinamento das redes neurais é minimizar a função de custo/erro (\textit{loss}) definida para a aplicação. Pode-se usar um conjunto de validação para salvar e usar os pesos do modelo que obter a melhor métrica no conjunto de validação, pois o modelo que obtiver a menor função de erro no treinamento não necessariamente será aquele que obterá a melhor métrica no conjunto de teste usado para avaliação do modelo. Para que este objetivo seja atingido, normalmente são usados otimizadores. O método clássico de descida do gradiente estocástico (\acrshort{SGD}) consiste basicamente em usar amostras aleatórias (\textit{mini-batch} para aproximar o verdadeiro gradiente que levará à minimização da função de custo/erro escolhida. O \acrshort{SGD} mantém uma única \textit{learning rate} (não muda durante o treino) para todas as atualizações de peso. O \textit{Adam}~\cite{kingma2014adam} é um otimizador que adapta a \textit{learning rate} baseando-se na média do primeiro momento e do segundo momento dos gradientes e na média móvel exponencial do gradiente e da raiz quadrada dele. Os parâmetros utilizados neste otimizador controlam as taxas de decaimento destas médias móveis.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Autoencoders}
Um Autoencoder (\acrshort{AE}) é uma classe de redes neurais que são formados por duas redes conectadas: um \textbf{encoder} e um \textbf{decoder}.
\begin{itemize}
\item O \textbf{encoder} tem como função converter a informação da entrada em uma representação menor e mais densa chamada de espaço latente. Pode ser representado como uma função de $x$, $f(x) = h$.
\item O \textbf{decoder}, por sua vez, tenta reconstruir a informação original, passando do espaço latente criado pelo encoder para o espaço original da informação. Pode ser representado como uma função de $h$, $g(h) = \hat{x}$
\end{itemize} 
Uma rede do tipo \acrshort{AE} é treinada de forma não supervisionada e pode ser descrita como $g(f(x)) = \hat{x}$. Normalmente, o objetivo é apenas diminuir a diferença entre $x$ e $\hat{x}$ (nesse caso, a função de custo a ser minimizada normalmente é a $MSE(x, \hat{x})$~[\refEq{mse}]). A camada entre o \textit{encoder} e o \textit{decoder} que contém menos neurônios~[\refFig{ae_architecture}] e força o \textit{encoder} a comprimir informação da representação original da entrada original gerando o espaço latente, denominado de \textit{bottleneck} (camada de gargalo). 
\figura[!htb]{ae_architecture}{Ilustração de um \textit{autoencoder} retirado de~\cite{fig:ae}}{ae_architecture}{width=\textwidth}
O problema fundamental com autoencoders como modelo gerativo é que o espaço latente para qual as entradas são convertidas pode não ser contínuo ou permitem fácil interpolação~[\refFig{latent_space}].
\figura[!htb]{latent_space}{Exemplo de espaço latente de um dataset de dígitos de numeros. Nota-se que, devido à descontinuidades, otimizar um AE apenas pelo erro de reconstrução não é bom quando é necessário fazer mais do que apenas replicar a mesma imagem, como gerar novas imagens ou alterações delas. Fonte:~\cite{fig:latent_space}}{latent_space}{width=\textwidth}

Para o problema de compressão de imagens, normalmente usa-se um binarizador na camada de gargalo com o objetivo de binarizar o latente gerado pelo \textit{encoder}. Assim, o \textit{encoder} é forçado a comprimir informação e o \textit{decoder} a diminuir a distorção usando menos informação. O binarizador transforma os valores em ponto flutuante (representação limitada dos números reais no computador) em inteiros que serão binarizados. Ele é utilizado para reduzir o espaço consumido pela imagem codificada, visto que números em pontos flutuante com precisão simples ocupam 32 bits o que levaria a uma alta taxa de bits por pixel. A avaliação dos modelos usados para este tipo de problema é dada considerando não só a taxa mas também métricas visuais que calculam o nível de distorção da imagem reconstruída. Existem três tipos de métricas visuais comumente utilizadas:
\begin{enumerate}
    \item \textit{\acrshort{PSNR}} definida por \equacao{psnr}{20 \cdot \left(\dfrac{MAX_{I}}{\sqrt{MSE}}\right)}, onde $MAX$ indica o maior valor possível para o pixel em uma imagem. Quando estes são representados em bits, usa-se $MAX_I = 2^B - 1$;
    \item \textit{\acrshort{SSIM}}~\cite{wang2004image}. Seja $x = \{x_i|i = 1,2,\dots,N\}$ e $y = \{y_i|i = 1,2,\dots,N\}$ dois sinais discretos não negativos e $\mu_x, \sigma_x^2$ e $\sigma_{xy}$ serem a média de $x$, a variância de $x$ e a covariância de $x$ e $y$, respectivamente. A \acrshort{SSIM} é dada por:
    \equacao{SSIM}{SSIM(x, y) = \dfrac{(2\mu_x\mu_y + C_1)(2\mu_x\mu_y + C_2)}{(\mu_x^2 + \mu_y^2 + C_1)(\mu_x^2 + \mu_y^2 + C_2)},} onde $C_1 = {(K_1L)}^2$, $C_2 = {(K_2L)}^2$ e $C_3 = C_2/2$. $L$ é o intervalo dinâmico dos valores dos pixels ($L = 255$ para 8 bits por pixel) e $K_1, K_2$ são constantes.
    %$$l(x,y) = \dfrac{2\mu_x\mu_y + C_1}{\mu_x^2 + \mu_y^2 + C_1}$$;
    %$$c(x, y) = \dfrac{2\mu_x\mu_y + C_2}{\mu_x^2 + \mu_y^2 + C_2}$$;
    %$$s(x, y) = \dfrac{\mu_{xy} + C_3}{\mu_x\mu_y + C_3}$$;
    \item \textit{\acrshort{MS-SSIM}}~\cite{wang2003multiscale} é baseada na \acrshort{SSIM}: 
    \equacao{MS-SSIM}{MS-SSIM(x,y) = {[l_M(x,y)]}^{\alpha M} \cdot \prod_{j=1}^{M}{[c_j(x,y)}^{\beta_j}{[s_j(x,y)]}^{\gamma_j},} onde os expoentes são usados para ajustar a importância relativa de cada componente e $l$, $c$ e $s$ são componentes de luminância, contraste e estrutura, respectivamente.
\end{enumerate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Compressão de Imagens com Taxa Variável usando Redes Neurais Recorrentes}
\label{variablerate}
Nesse trabalho~\cite{toderici2015variable} é descrito um autoencoder geral baseado em redes neurais para compressão de imagens. São apresentadas três instanciações desta arquitetura: \emph{Long Short Term Memory (LSTM)}~\cite{gers1999learning}, \textit{Fully-Connected} e Convolucional. Para cada arquitetura, existe uma função \textit{E} que pega o \textit{patch} da imagem como entrada e produz uma representação codificada. Esta representação é passada para uma função de binarização \textit{B}, que é a mesma para todas redes. $B$ usa tangente hiperbólica (\textit{tanh}) como função de ativação antes de realizar a binarização, de forma a gerar um $x$ tal que $x \in [-1, 1]$. A seguir, é calculado $b(x)$: \equacao{bin}{b(x) = x + \epsilon \,\in \{-1, 1\},} \equacao{noise}{\epsilon \sim \begin{cases} 
1 - x\, \text{ com probabilidade } \frac{1+x}{2},\\
-1 - x\, \text{ com probabilidade } \frac{1-x}{2},\\
\end{cases}} onde $\epsilon$, corresponde ao ruído de quantização. A função completa de binarização usada na camada de gargalo é: \equacao{binarizer}{B(x) = b(tanh(W^{bin}x + b^{bin})).}
$W^{bin}$ e $b^{bin}$ são os pesos e bias padrões das camadas anteriores da rede. Esta formulação para a binarização é usada em todos os modelos para a \textit{forward pass} da rede. Para a \textit{backward pass} da \textit{back-propagation}, é usada a derivada da esperança. Visto que a esperança de $b(x)$ será igual a $x$ para todo $x$, os gradientes serão passados por $b$ sem mudanças. Essa binarização é aplicada somente em tempo de treino. Em tempo de teste, $b$ é substituída por \equacao{binf}{b^{inf}(x) = \begin{cases}
-1,\, \text{ se } x < 0,\\
1,\, \text{ caso contrário. }\\
\end{cases}}

Finalmente, para cada arquitetura é considerada um função decodificadora $D$, que pega a representação binária produzida 
por $B$ e gera a reconstrução de um \textit{patch}. Esses três componentes formam um \textit{autoencoder}: $\hat{x} = D(B(E(x)))$, que é a base de todos os modelos de compressão apresentados.

% Para ser possível transmitir informação incremental, é utilizado um conceito de %várias arquiteturas encadeadas~[\refFig{figura Toderici}] onde o objetivo da primeira %será reconstruir a entrada original gerando um resíduo $r_t$ que será dado como %entrada para a próxima arquitetura cujo objetivo será reconstruir este resíduo $r_t$ e %assim por diante. Formalmente, é encadeado múltiplas cópias do \textit{autoencoder} %residual %$F_t$ definido como: \equacao{residuos}{F_t(r_{t-1}) = %D_t(B(E_t(r_{t-1}))).}, onde %$r_0$ é igual ao \textit{patch} de entrada original e %$r_t, t > 0$ representa o erro %residual após $t$ estágios. A reconstrução final é %calculada somando-se todos os %resíduos e cada estágio é penalizado pela diferença %entre a predição e o resíduo %antigo: $r_t = F_t(r_{t-1}) - r_{t-1}.$. A rede completa %é treinada minimizando %${\|r_t\|}_2,\, \text{ para } t = 1,\cdot,N$, onde $N$ é o %número total de %\textit{autoencoders} residuais no modelo.

No trabalho posterior, Toderici~\cite{toderici2017full} mostrou em seus resultados que o efeito de usar um conjunto de treinamento de alta entropia\footnote{Em processamento de imagens entropia diz respeito à quantidade de informação na imagem. Alta entropia, pode ser visto como maior variância nos valores dos pixels} é benéfico para a rede, dada a importância de treinar modelos de \textit{machine learning} com exemplos difíceis. No seu trabalho, ele definiu o conjunto de alta entropia como sendo o conjunto formado por imagens que difíceis de comprimir pelo \acrshort{PNG} (método de compressão sem perdas de imagens), ou seja, os arquivos no formato \acrshort{PNG} com o maior número de bytes. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Compressão de Imagens com Perdas usando Autoencoders Compressivos}
Este trabalho~\cite{theis2017lossy} alcança performance similar ou superior ao \textit{JPEG2000} quando avaliado na qualidade visual. Entretanto, ao contrário do \textit{JPEG2000} o framework proposto pode ser otimizado para classes de imagens específicas como miniaturas ou imagens não-naturais, métricas arbitrárias e é facilmente generalizável para outras formas de mídia. Essa performance é alcançada usando arquiteturas de redes neurais eficientes que permitem uma decodificação quase em tempo real de grandes imagens mesmo em dispositivos de baixa potência. Para isso, inspirado no trabalho de~\cite{shi2016real} a maior parte das convoluções são realizadas em um espaço reduzido para aumentar a velocidade da computação. O \textit{upsample} é realizado usando convoluções seguidas por uma remodelação dos coeficientes (subpix na~\refFig{cae}).
\figura[!htb]{cae}{Ilustração do autoencoder compressivo usado no paper. A notação $C \times K \times K$ significa convoluções $K \times K$ com $C$ filtros. Fonte:~\cite{theis2017lossy}}{cae}{width=\textwidth}

O autoencoder compressivo possui três componentes: um codificador $f$, um decodificador $g$ e um modelo probabilístico $Q$:
$$f :  \mathbb{R}^N \to \mathbb{R}^M\textrm{,}\,\,\, g : \mathbb{R}^M \to \mathbb{R}^N\textrm{, }\,\,\, Q : \mathbb{Z}^M \to [0, 1]$$
A distribuição de probabilidade discreta, definida por $Q$ é usada para mapear o número de bits à suas representações baseado nas suas frequências, isto é, codificação de entropia. O objetivo é otimizar o balanceamento (\textit{trade-off}) entre usar um número pequeno de bits e ter pouca distorção. A principal fonte de perda de informação é a quantização. A saída quantizada do codificador usada para representar uma imagem é salva sem perdas. Informação adicional pode ser descartada pelo codificador, e o decodificador pode não ser capaz de decodificar a informação disponível de forma perfeita, aumentando a distorção.

Os autores discutem duas funções principais para realizar a quantização: arredondamento estocástico e arredondamento para o inteiro mais próximo.

Para a realização de arredondamento estocástico é usada a seguinte equação: \equacao{cae_round}{\{y\} \approx \lfloor y \rfloor + \epsilon,\,\,\, \epsilon \in \{0, 1\},\,\,\, P(\epsilon = 1)\, =\, y - \lfloor y \rfloor,} 
A derivada da função chão é zero em todos os lugares, exceto nos inteiros, onde ela é indefinida. Por isso, a derivada na passagem de volta (\textit{backward pass}) usada no algoritmo de backpropagation~\cite{rumelhart1988learning} é substituída com a derivada da esperança: \equacao{back_cae}{\dfrac{d}{dy}\{y\} := \dfrac{d}{dy}E[\{y\}] = \dfrac{d}{dy} = 1.}

A função de arredondamento também é não-diferenciável, por isso na \textit{backward pass} os gradientes são passados sem modificação do \textit{decoder} para o encoder \textit{encoder}. A quantização é realizada normalmente na \textit{forward pass}, visto que substituir o arredondamento por uma aproximação completamente pode levar o \textit{decoder} a aprender à inverter essa aproximação, removendo a informação da \textit{bottleneck} (camada de gargalo) que força a rede a comprimir informação.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Redes Neurais Convolucionais}
Em \textit{deep learning} (aprendizado profundo), uma rede neural convolucional (\acrshort{CNN})~\cite{lecun2010convolutional} é uma classe de redes neurais profundas que usa convoluções para detectarem características e padrões presentes nas imagens. As primeiras camadas detectam características que podem ser reconhecidas e interpretadas de maneira relativamente fácil. Camadas posteriores detectam características mais abstratas e usualmente presentes em muitas das características detectadas por camadas anteriores, conforme mostra a~\refFig{convnet}. A arquitetura de uma \acrshort{CNN} é análoga ao padrão de conectividade dos neurônios no cérebro humano e foi inspirada pela organização do córtex visual. Neurônios individuais respondem à estímulos apenas em uma região restrita do campo visual que é conhecida como campo receptivo. Uma coleção de sobreposição desses campos cobrem toda a área visual~\cite{livrodl}.

\figura[!htb]{convnet}{Exemplo mostrando a saída de cada camada de uma rede neural convolucional. Fonte:~\cite{fig:conv}}{convnet}{width=\textwidth}

Uma \acrshort{CNN} é capaz de capturar depêndencias espaciais e temporais na imagem de forma bem-sucedida através da aplicação de filtros relevantes e dispensa a necessidade de engenharia de características. A arquitetura realiza um melhor ajuste ao conjunto de imagens devido à redução do número de parâmetros envolvidos e a reusabilidade dos pesos. Em outras palavras, a rede pode ser treinada para entender melhor a complexidade da imagem e suas características relevantes. Esta extração de características é feita por meio da aplicação de filtros\footnote{Filtro é um termo que vem de processamento no domínio da frequência e se refere a aceitar ou rejeitar certos componentes de frequência} no domínio do espaço denominados convoluções. Para uma máscara (filtro) de tamanho $m \times n$, $m = 2a + 1$ e $n = 2b + 1$, onde $a$ e $b$ são inteiros positivos. Para qualquer ponto $(x,\, y)$ na imagem $f$, a resposta do filtro é a soma dos produtos dos coeficientes do filtro e dos pixels da imagem englobados pelo filtro~(\refEq{conv:eq1}).
\equacao{conv:eq1}{g(x,\, y) \,=\, \sum_{s\, =\, -a}^{a}\, \sum_{t\, =\, -b}^{b}{w(s,t)f(x + s, y + t)}}
Conforme mostra a~\refFig{conv}, $x$ e $y$ variam de modo que cada pixel em $w$ visite todos os pixels em $f$.
\figura[!htb]{conv}{Ilustração da operação de filtragem no domínio do espaço (convolução). $w$ é o kernel do filtro e $f$ é a área da imagem coberta pelo filtro. Fonte:~\cite{fig:zaghetto}}{conv}{width=\textwidth}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 