Os recursos requeridos para armazenar e transmitir imagens são imensos, o que torna a sua compressão desejável. Todos os esforços feitos em algoritmos de compressão de imagens clássicos abordam o problema de compressão de um ponto de vista empírico: humanos desenvolvem várias heurísticas para reduzir a quantidade de informação necessária para representar a imagem explorando imperfeições no sistema visual humano, de modo que seja possível reconstruí-la sem muita perda de informação. 

O sucesso das redes neurais convolucionais profundas (CNNs) em visão computacional tem inspirado pesquisadores da comunidade de compressão de imagens a tentar desenvolver algoritmos que aprendem com os dados, em vez de confiar no conhecimento de especialistas. Redes do tipo autoencoder vem sendo utilizadas nesses algoritmos, visto que redução de dimensionalidade faz parte do funcionamento delas. Até agora esses algoritmos não levaram a uma melhoria significativa em relação aos codecs clássicos.

O objetivo do presente trabalho é estudar e explorar soluções usando \textit{autoencoders} convolucionais recorrentes para o desafio de comprimir imagens, buscando propor um método que possa ser competitivo em relação aos codecs clássicos. Para isso, foram testados e avaliados os métodos clássicos \textit{JPEG} e \textit{JPEG2000} que foram os primeiros codecs vastamente utilizados. Esses métodos foram comparados, em bases de dados comumente usadas para este tipo de problema, com o framework de compressão de imagens usando autoencoders convolucionais desenvolvido.