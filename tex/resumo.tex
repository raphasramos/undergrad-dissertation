Os recursos requeridos para armazenar e transmitir imagens são imensos, o que torna a sua compressão necessária. Todos os esforços feitos em algoritmos de compressão de imagens clássicos abordam o problema de compressão de um ponto de vista empírico: humanos desenvolvem várias heurísticas para reduzir a quantidade de informação necessária para representar a imagem explorando imperfeições no sistema visual humano, de modo que seja possível reconstruí-la sem muita perda de informação. 

O sucesso das redes neurais convolucionais profundas (CNNs) em visão computacional tem inspirado pesquisadores da comunidade de compressão de imagens a tentar desenvolver algoritmos que aprendam com os dados, em vez de confiar no conhecimento de especialistas. Até agora, esses algoritmos não levaram a uma melhoria inquestionável em relação aos codecs clássicos.

O objetivo do presente trabalho é estudar e explorar soluções usando \textit{autoencoders} convolucionais para o desafio de comprimir imagens, buscando propor um método que possa ser competitivo em relação aos codecs clássicos. Para isso, foram testados e avaliados os métodos de compressão clássico \textit{JPEG} e \textit{JPEG2000}. Esses métodos foram comparados com o framework de compressão de imagens usando autoencoders convolucionais desenvolvido em bases de dados comumente usadas para este tipo de problema. 


